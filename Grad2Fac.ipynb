{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0207a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user_dir = os.path.expanduser(\"~\")\n",
    "os.environ[\"R_HOME\"] = \"{}/anaconda3/envs/gpr/Lib/R\".format(user_dir)\n",
    "os.environ[\"PATH\"]   = \"{}/anaconda3/envs/gpr/Lib/R/bin/x64\".format(user_dir) + \";\" + os.environ[\"PATH\"]\n",
    "from advanced_pca import CustomPCA\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold \n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.io as pio\n",
    "import math\n",
    "from wordcloud import WordCloud\n",
    "from plotly.express.colors import sample_colorscale\n",
    "pio.templates.default = \"plotly_dark\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b98ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_previous_pcas = False\n",
    "data=pd.read_csv('output_PCA.csv') # Reading datafile (should be in the same directory as our IDE)\n",
    "try:\n",
    "    #use_previous_pcas = True\n",
    "    PCAdata = data.drop([\"Participant #\",\"Runtime_mod\",\"Task_name\",\"Gradient 1\",\"Gradient 2\",\"Gradient 3\",\"FAC1_1\",\"FAC2_1\",\"FAC3_1\",\"FAC4_1\"],axis=1) # Getting rid of unneeded columns for PCA\n",
    "    PCAresults = data[[\"FAC1_1\",\"FAC2_1\",\"FAC3_1\",\"FAC4_1\"]]\n",
    "    PCAresults = PCAresults.rename(columns={\"FAC1_1\": 0, \"FAC2_1\": 1,\"FAC3_1\":2,\"FAC4_1\":3})\n",
    "    loadings = pd.read_csv(\"new_loadings.csv\",header=None,index_col=0).rename(columns={1:\"Component 0\", 2:\"Component 1\", 3:\"Component 2\", 4:\"Component 3\"})\n",
    "except:\n",
    "    use_previous_pcas = True\n",
    "    PCAdata = data.drop([\"Participant#\",\"Runtime_mod\",\"Task_name\",\"Gradient1\",\"Gradient2\",\"Gradient3\",\"FAC1_1\",\"FAC2_1\",\"FAC3_1\",\"FAC4_1\"],axis=1) # Getting rid of unneeded columns for PCA\n",
    "    data = data.rename(columns={\"Gradient1\": \"Gradient 1\", \"Gradient2\": \"Gradient 2\",\"Gradient3\":\"Gradient 3\"})\n",
    "    PCAresults = data[[\"FAC1_1\",\"FAC2_1\",\"FAC3_1\",\"FAC4_1\"]]\n",
    "    PCAresults = PCAresults.rename(columns={\"FAC1_1\": 0, \"FAC2_1\": 1,\"FAC3_1\":2,\"FAC4_1\":3})\n",
    "    loadings = pd.read_csv(\"old_loadings.csv\",header=None,index_col=0).rename(columns={1:\"Component 0\", 2:\"Component 1\", 3:\"Component 2\", 4:\"Component 3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c93044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2863.4898796646885, 0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_square_value,p_value=calculate_bartlett_sphericity(PCAdata) # I belive this checks for correlations within our dataset which would make a PCA weird\n",
    "chi_square_value, p_value # significant p-value means we're ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c090f7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.85585898, 0.76201432, 0.61533417, 0.80801207, 0.81901643,\n",
       "        0.72690386, 0.78571716, 0.72342191, 0.71980541, 0.74463419,\n",
       "        0.74314689, 0.80242482, 0.7687802 , 0.71078397, 0.79430761,\n",
       "        0.77720446]),\n",
       " 0.7636127234923935)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmo_all,kmo_model=calculate_kmo(PCAdata) # Not even going to pretend I understand the Kaiser-Meyer-Olkin criteria\n",
    "kmo_all,kmo_model # kmo_model > 0.6 is acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf25956",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_previous_pcas == False:\n",
    "    PCAmodel = CustomPCA(n_components=4,rotation=\"varimax\") \n",
    "    PCAmodel.fit(PCAdata)\n",
    "    loadings = PCAmodel.components_\n",
    "\n",
    "    # for en,row in enumerate(loadings):\n",
    "    #     if np.mean(row) < 0:\n",
    "    #         loadings[en] = np.multiply(row, -1)\n",
    "    \n",
    "    names = PCAdata.columns\n",
    "    loadings = pd.DataFrame(np.round(loadings.T,3),index=names,columns=[\"Component 0\",\"Component 1\",\"Component 2\",\"Component 3\"])\n",
    "    PCAresults = PCAmodel.transform(PCAdata).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad92f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAC=np.asarray([PCAresults[0], PCAresults[1],PCAresults[2],PCAresults[3]]).T\n",
    "GRAD=np.asarray([data[\"Gradient 1\"], data[\"Gradient 2\"],data[\"Gradient 3\"]]).T\n",
    "KeepIndex=~np.isnan(FAC[:,0])\n",
    "tasknum = len(data[\"Task_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ce63cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Tasklabels,Taskindices=np.unique(data[\"Task_name\"],return_inverse=True)\n",
    "FAC_TaskCentres=np.zeros([tasknum,4])\n",
    "for i in range(tasknum):\n",
    "    FAC_TaskCentres[i,:]=FAC[Taskindices==i,:].mean(axis=0)\n",
    "\n",
    "Grad_TaskCentres=np.zeros([tasknum,3])\n",
    "for i in range(tasknum):\n",
    "\n",
    "    Grad_TaskCentres[i,:]=GRAD[np.ix_(Taskindices==(i),[0,1,2])].mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3ca4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit whole dataset\n",
    "\n",
    "\n",
    "\n",
    "fig_estimated_mean = make_subplots(rows=4, cols=2,column_widths=[0.7, 0.25], vertical_spacing=0,horizontal_spacing=0.2, subplot_titles=([\"Component 1\",\"Component 1 Wordcloud\", \"Component 2\",\"Component 2 Wordcloud\", \"Component 3\",\"Component 3 Wordcloud\", \"Component 4\",\"Component 4 Wordcloud\"]),specs=[[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}]])\n",
    "\n",
    "fig_standard_deviation = make_subplots(rows=4, cols=2,column_widths=[0.7, 0.25], vertical_spacing=0,horizontal_spacing=0.2,subplot_titles=([\"Component 1\",\"Component 1 Wordcloud\", \"Component 2\",\"Component 2 Wordcloud\", \"Component 3\",\"Component 3 Wordcloud\", \"Component 4\",\"Component 4 Wordcloud\"]),specs=[[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}]])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "\n",
    "    standardscaler=StandardScaler()\n",
    "    X=Grad_TaskCentres\n",
    "\n",
    "    y = standardscaler.fit_transform(FAC_TaskCentres[:,i].reshape(-1,1))    \n",
    "\n",
    "\n",
    "    kernel = 1.0 * Matern(length_scale=0.5, length_scale_bounds=(0.5, 1), nu=2.5) + WhiteKernel(noise_level_bounds=[0.001,0.1],noise_level=0.05)\n",
    "\n",
    "\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, random_state=3,normalize_y=False,alpha=0)\n",
    "    \n",
    "    @ignore_warnings(category=ConvergenceWarning)\n",
    "    def _f():\n",
    "        gpr.fit(X, y)\n",
    "\n",
    "    _f()\n",
    "\n",
    "\n",
    "    lim = 0.6\n",
    "    res = 30\n",
    "    lin = np.linspace(-lim, lim, res)\n",
    "\n",
    "    \n",
    "    x1, x2, x3 = np.meshgrid(lin, lin, lin)\n",
    "\n",
    "    xx = np.vstack((x1.flatten(), x2.flatten(), x3.flatten())).T\n",
    "\n",
    "    y_mean, y_sd = gpr.predict(xx, return_std=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    fig_estimated_mean.add_trace(go.Volume(\n",
    "        x=pd.Series(x1.flatten(),name=\"Gradient 1\"),\n",
    "        y=pd.Series(x2.flatten(),name=\"Gradient 2\"),\n",
    "        z=pd.Series(x3.flatten(),name=\"Gradient 3\"),\n",
    "        value=y_mean,\n",
    "        \n",
    "        hoverinfo='skip',\n",
    "        opacityscale=[[0, 0.8], [0.35, 0],[0.65, 0], [1, 0.8]],\n",
    "        surface_count=25,\n",
    "        showlegend=False,\n",
    "        colorscale='RdBu',\n",
    "        colorbar={\"tickmode\":\"array\",'tickvals': [min(y_mean),max(y_mean)],'ticktext': [\"Predicted low loading\",\"Predicted high loading\"]}\n",
    "\n",
    "        ),i+1,1)\n",
    "\n",
    "    fig_estimated_mean.update(\n",
    "        layout_scene=dict(\n",
    "            xaxis_title=\"Gradient 1\",\n",
    "            yaxis_title=\"Gradient 2\",\n",
    "            zaxis_title=\"Gradient 3\",\n",
    "            aspectmode='data'\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig_estimated_mean.add_trace(go.Scatter3d(\n",
    "        x=Grad_TaskCentres[:,0], \n",
    "        y=Grad_TaskCentres[:,1],\n",
    "        z=Grad_TaskCentres[:,2],\n",
    "        marker_color=FAC_TaskCentres[:,i],\n",
    "        marker_colorscale='RdBu',\n",
    "        text=Tasklabels,mode=\"markers+text\",\n",
    "        showlegend=False,\n",
    "        \n",
    "        ),i+1,1)\n",
    "\n",
    "    \n",
    "\n",
    "    fig_standard_deviation.add_trace(go.Volume(\n",
    "        x=pd.Series(x1.flatten(),name=\"Gradient 1\"),\n",
    "        y=pd.Series(x2.flatten(),name=\"Gradient 2\"),\n",
    "        z=pd.Series(x3.flatten(),name=\"Gradient 3\"),\n",
    "        value=y_sd,\n",
    "        hoverinfo='skip',\n",
    "        showlegend=False,\n",
    "        colorscale='RdBu',\n",
    "        #showscale=False,\n",
    "        opacityscale=[[0, 0.8], [0.35, 0],[0.65, 0], [1, 0.8]],\n",
    "        colorbar={\"tickmode\":\"array\",'tickvals': [min(y_sd),max(y_sd)],'ticktext': [\"Low uncertainty in predicted loading\",\"High uncertainty in predicted loading\"]},\n",
    "        surface_count=25,\n",
    "        ),i+1,1)\n",
    "    \n",
    "    fig_standard_deviation.update(\n",
    "        layout_scene=dict(\n",
    "            xaxis_title=\"Gradient 1\",\n",
    "            yaxis_title=\"Gradient 2\",\n",
    "            zaxis_title=\"Gradient 3\",\n",
    "            aspectmode='data'\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    unscaledloadings = loadings[[\"Component {}\".format(i)]]\n",
    "    unscaledloadings= unscaledloadings.sort_values(by=\"Component {}\".format(i),ascending=False)\n",
    "    scaledloadings = MinMaxScaler().fit_transform(unscaledloadings).flatten().round(4)\n",
    "    scaledloadings_col = MinMaxScaler(feature_range=(-10,10)).fit_transform(unscaledloadings).round(4)\n",
    "    scaledloadings_col = pd.DataFrame(scaledloadings_col.T).apply(lambda x: 1 / (1 + math.exp(-x))).to_numpy()\n",
    "    colours = sample_colorscale(\"RdBu\",samplepoints=scaledloadings_col)\n",
    "    colour_dict = {x.split(\"_\")[0]:y for x,y in zip(unscaledloadings.index,colours)}\n",
    "    \n",
    "    absolutescaledloadings = np.where(scaledloadings < 0.5, 1-scaledloadings,scaledloadings)\n",
    "    rescaledloadings = MinMaxScaler().fit_transform(absolutescaledloadings.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    freq_dict = {x.split(\"_\")[0]:y for x,y in zip(unscaledloadings.index,rescaledloadings)}\n",
    "    def color_func(word, *args, **kwargs): #colour function to supply to wordcloud function.. don't ask !\n",
    "        \n",
    "        return colour_dict[word]\n",
    "    \n",
    "    wc = WordCloud(background_color=\"white\", color_func=color_func, \n",
    "                width=400, height=400, prefer_horizontal=1, \n",
    "                min_font_size=8, max_font_size=200\n",
    "                )\n",
    "    # generate wordcloud from loadings in frequency dict\n",
    "    wc = wc.generate_from_frequencies(freq_dict)\n",
    "    im = wc.to_image()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #im = Image.open(\"clouds/{}.png\".format(i+1))\n",
    "    fig_estimated_mean.add_layout_image(\n",
    "            dict(\n",
    "                source=im,\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x=0,\n",
    "                y=0,\n",
    "                xanchor=\"center\",\n",
    "                yanchor=\"middle\",\n",
    "                sizex=2,\n",
    "                sizey=2,\n",
    "                \n",
    "                ),row=i+1,col=2\n",
    "    )\n",
    "    fig_estimated_mean.update_xaxes(range=[-1,1],showticklabels=False,row=i+1,col=2,showgrid=False,zeroline=False)\n",
    "    fig_estimated_mean.update_yaxes(range=[-1,1],showticklabels=False,row=i+1,col=2,showgrid=False,zeroline=False)\n",
    "\n",
    "    fig_standard_deviation.add_layout_image(\n",
    "            dict(\n",
    "                source=im,\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x=0,\n",
    "                y=0,\n",
    "                xanchor=\"center\",\n",
    "                yanchor=\"middle\",\n",
    "                sizex=2,\n",
    "                sizey=2,\n",
    "                \n",
    "                ),row=i+1,col=2\n",
    "    )\n",
    "    fig_standard_deviation.update_xaxes(range=[-1,1],showticklabels=False,row=i+1,col=2,showgrid=False,zeroline=False)\n",
    "    fig_standard_deviation.update_yaxes(range=[-1,1],showticklabels=False,row=i+1,col=2,showgrid=False,zeroline=False)\n",
    "    \n",
    "    if i == 0:\n",
    "        figtemp = make_subplots(rows=1, cols=2, subplot_titles=([\"Component 1\",\"Component 1 Wordcloud\"]),specs=[[{'type': 'surface'},{'type':'xy'}]])\n",
    "        figtemp.add_trace(go.Volume(\n",
    "            x=x1.flatten(),\n",
    "            y=x2.flatten(),\n",
    "            z=x3.flatten(),\n",
    "            value=y_mean,\n",
    "            \n",
    "            hoverinfo='skip',\n",
    "            opacityscale=[[0, 0.8], [0.35, 0],[0.65, 0], [1, 0.8]],\n",
    "            surface_count=25,\n",
    "            showlegend=False,\n",
    "            colorbar={\"tickmode\":\"array\",'tickvals': [min(y_mean),max(y_mean)],'ticktext': [\"Predicted low loading\",\"Predicted high loading\"]}\n",
    "            )\n",
    "                            )\n",
    "        figtemp.add_trace(go.Scatter3d(\n",
    "            x=Grad_TaskCentres[:,0], \n",
    "            y=Grad_TaskCentres[:,1],\n",
    "            z=Grad_TaskCentres[:,2],\n",
    "            marker_color=FAC_TaskCentres[:,i],\n",
    "            text=Tasklabels,mode=\"markers+text\",\n",
    "            showlegend=False,\n",
    "            ))\n",
    "        figtemp.update(\n",
    "            layout_scene=dict(\n",
    "                xaxis_title=\"Simplicity\",\n",
    "                yaxis_title=\"Magical Mystery Gradient\",\n",
    "                zaxis_title=\"Task-Disinterested\",\n",
    "            ),\n",
    "        )\n",
    "        im = Image.open(\"clouds/1.png\")\n",
    "        figtemp.add_layout_image(\n",
    "                dict(\n",
    "                    source=im,\n",
    "                    xref=\"x\",\n",
    "                    yref=\"y\",\n",
    "                    x=0,\n",
    "                    y=0,\n",
    "                    xanchor=\"center\",\n",
    "                    yanchor=\"middle\",\n",
    "                    sizex=1,\n",
    "                    sizey=1,\n",
    "                    #sizing=\"stretch\",\n",
    "                    ),row=1,col=2\n",
    "        )\n",
    "        figtemp.update_xaxes(range=[-1,1],showticklabels=False,row=1,col=2)\n",
    "        figtemp.update_yaxes(range=[-1,1],showticklabels=False,row=1,col=2)\n",
    "        \n",
    "        figtemp.write_html(\"estimated_mean_single.html\")\n",
    "        fig_estimated_mean.update(layout_coloraxis_showscale=False)\n",
    "\n",
    "fig_estimated_mean.write_html(\"estimated_mean.html\",default_width=2000,default_height=5000)\n",
    "fig_standard_deviation.write_html(\"standard_deviation.html\",default_width=2000,default_height=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34fd07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "k = 4\n",
    "kernel = 1.0 * Matern(length_scale=0.5, length_scale_bounds=(0.5, 1), nu=2.5) + WhiteKernel(noise_level_bounds=[0.001,0.5],noise_level=0.05)\n",
    "#kernel = 1.0 * Matern(length_scale=0.5, length_scale_bounds=(0.5, 1), nu=2.5)+ WhiteKernel(noise_level_bounds=[0.001,0.5],noise_level=0.05)\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=None,normalize_y=True,alpha=0.1)\n",
    "X=Grad_TaskCentres\n",
    "scores = {}\n",
    "\n",
    "for PCnum in range(FAC_TaskCentres.shape[1]):\n",
    "    standardscaler=StandardScaler()\n",
    "\n",
    "    y = FAC_TaskCentres[:,PCnum]\n",
    "    \n",
    "    @ignore_warnings(category=ConvergenceWarning)\n",
    "    def ___f():\n",
    "        score_gradfac, perm_scores_gradfac, pvalue_gradfac = permutation_test_score(gpr, X, y, scoring=\"neg_mean_absolute_error\", cv=kf, n_permutations=1000,n_jobs=-1)\n",
    "    \n",
    "        scores.update({PCnum:[score_gradfac,pvalue_gradfac]})\n",
    "    \n",
    "    ___f()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8a4b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Results for component 0:\n",
      "        Largest loadings: \n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "Distracting_response    0.745\n",
       "Future_response         0.721\n",
       "Intrusive_response      0.610\n",
       "Focus_response          0.459\n",
       "Self_response           0.383\n",
       "Name: Component 0, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Permutation test score: -0.3357646872920018,\n",
      "        Permutation test significance: 0.03596403596403597\n",
      "        \n",
      "\n",
      "        Results for component 1:\n",
      "        Largest loadings: \n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "Detailed_response      0.773\n",
       "Deliberate_response    0.687\n",
       "Absorption_response    0.606\n",
       "Knowledge_response     0.535\n",
       "Problem_response       0.420\n",
       "Name: Component 1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Permutation test score: -0.248045602883238,\n",
      "        Permutation test significance: 0.5194805194805194\n",
      "        \n",
      "\n",
      "        Results for component 2:\n",
      "        Largest loadings: \n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "Past_response         0.696\n",
       "Other_response        0.692\n",
       "Problem_response     -0.604\n",
       "Self_response         0.519\n",
       "Knowledge_response    0.387\n",
       "Name: Component 2, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Permutation test score: -0.31212042516578525,\n",
      "        Permutation test significance: 0.006993006993006993\n",
      "        \n",
      "\n",
      "        Results for component 3:\n",
      "        Largest loadings: \n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "Images_response         0.912\n",
       "Distracting_response    0.228\n",
       "Other_response         -0.220\n",
       "Focus_response         -0.190\n",
       "Self_response           0.180\n",
       "Name: Component 3, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Permutation test score: -0.2871095990833127,\n",
      "        Permutation test significance: 0.04895104895104895\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "numloadings = 5\n",
    "for score in scores:\n",
    "    try:\n",
    "        loading = loadings[\"Component {}\".format(score)]\n",
    "        loadingpos = loading.apply(lambda x: np.abs(x)).sort_values(ascending=False)\n",
    "        tops = loading[loadingpos[:numloadings].index]\n",
    "    except:\n",
    "        tops = \"Loadings from SPSS are unavailable\"\n",
    "    print(\n",
    "        \"\"\"\n",
    "        Results for component {}:\n",
    "        Largest loadings: \n",
    "        \"\"\".format(score)\n",
    "        )\n",
    "    with pd.option_context('display.max_rows', 5,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.width', 1000,\n",
    "                       'display.precision', 3,\n",
    "                       'display.colheader_justify', 'center'):\n",
    "        display(tops)\n",
    "    print(\n",
    "        \"\"\"\n",
    "        Permutation test score: {},\n",
    "        Permutation test significance: {}\n",
    "        \"\"\".format(scores[score][0],scores[score][1])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9549f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "#Assess out of sample prediction with 3d gradient\n",
    "fig_estimated_mean = make_subplots(rows=12, cols=2,column_widths=[0.7, 0.25], vertical_spacing=0,horizontal_spacing=0.2, subplot_titles=([\n",
    "    \"Component 1\",\"Component 1 Wordcloud\", \"Component 2\",\"Component 2 Wordcloud\", \"Component 3\",\"Component 3 Wordcloud\", \"Component 4\",\"Component 4 Wordcloud\"\n",
    "    ]),specs=[\n",
    "        [{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],\n",
    "        [{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],\n",
    "        [{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}]\n",
    "        ])\n",
    "\n",
    "fig_standard_deviation = make_subplots(rows=12, cols=2,column_widths=[0.7, 0.25], vertical_spacing=0,horizontal_spacing=0.2,subplot_titles=([\n",
    "    \"Component 1\",\"Component 1 Wordcloud\", \"Component 2\",\"Component 2 Wordcloud\", \"Component 3\",\"Component 3 Wordcloud\", \"Component 4\",\"Component 4 Wordcloud\"\n",
    "    ]),specs=[\n",
    "        [{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],\n",
    "        [{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],\n",
    "        [{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}],[{'type': 'surface'},{'type': 'xy'}]\n",
    "        ])\n",
    "combs = list(combinations(range(FAC_TaskCentres.shape[1]),3))\n",
    "ti = 0\n",
    "for i in range(Grad_TaskCentres.shape[1]):\n",
    "    for c in combs:\n",
    "        \n",
    "        standardscaler=StandardScaler()\n",
    "        X=FAC_TaskCentres[:,c]\n",
    "\n",
    "        y = standardscaler.fit_transform(Grad_TaskCentres[:,i].reshape(-1,1))    \n",
    "\n",
    "\n",
    "        kernel = 1.0 * Matern(length_scale=0.5, length_scale_bounds=(0.5, 1), nu=2.5) + WhiteKernel(noise_level_bounds=[0.001,0.1],noise_level=0.05)\n",
    "\n",
    "\n",
    "        gpr = GaussianProcessRegressor(kernel=kernel, random_state=3,normalize_y=False,alpha=0)\n",
    "        \n",
    "        @ignore_warnings(category=ConvergenceWarning)\n",
    "        def _f():\n",
    "            gpr.fit(X, y)\n",
    "\n",
    "        _f()\n",
    "\n",
    "\n",
    "        lim = 1.2\n",
    "        res = 20\n",
    "        lin = np.linspace(-lim, lim, res)\n",
    "\n",
    "        lins = [lin for x in range(X.shape[1])]\n",
    "        x_coords = np.meshgrid(*lins)\n",
    "        x_coords = [x.flatten() for x in x_coords]\n",
    "        xx = np.vstack(x_coords).T\n",
    "\n",
    "        y_mean, y_sd = gpr.predict(xx, return_std=True)\n",
    "        \n",
    "        x1,x2,x3 = x_coords\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        fig_estimated_mean.add_trace(go.Volume(\n",
    "            x=x1,\n",
    "            y=x2,\n",
    "            z=x3,\n",
    "            value=y_mean,\n",
    "            \n",
    "            hoverinfo='skip',\n",
    "            opacityscale=[[0, 0.8], [0.35, 0],[0.65, 0], [1, 0.8]],\n",
    "            surface_count=25,\n",
    "            showlegend=False,\n",
    "            colorscale='RdBu',\n",
    "            colorbar={\"tickmode\":\"array\",'tickvals': [min(y_mean),max(y_mean)],'ticktext': [\"Predicted low loading\",\"Predicted high loading\"]}\n",
    "\n",
    "            ),ti+1,1)\n",
    "\n",
    "        fig_estimated_mean.update(\n",
    "            layout_scene=dict(\n",
    "                xaxis_title=\"Gradient 1\",\n",
    "                yaxis_title=\"Gradient 2\",\n",
    "                zaxis_title=\"Gradient 3\",\n",
    "                aspectmode='data'\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        fig_estimated_mean.add_trace(go.Scatter3d(\n",
    "            x=FAC_TaskCentres[:,0], \n",
    "            y=FAC_TaskCentres[:,1],\n",
    "            z=FAC_TaskCentres[:,2],\n",
    "            marker_color=Grad_TaskCentres[:,i],\n",
    "            marker_colorscale='RdBu',\n",
    "            text=Tasklabels,mode=\"markers+text\",\n",
    "            showlegend=False,\n",
    "            \n",
    "            ),ti+1,1)\n",
    "\n",
    "        \n",
    "\n",
    "        fig_standard_deviation.add_trace(go.Volume(\n",
    "            x=x1,\n",
    "            y=x2,\n",
    "            z=x3,\n",
    "            value=y_sd,\n",
    "            hoverinfo='skip',\n",
    "            showlegend=False,\n",
    "            colorscale='RdBu',\n",
    "            #showscale=False,\n",
    "            opacityscale=[[0, 0.8], [0.35, 0],[0.65, 0], [1, 0.8]],\n",
    "            colorbar={\"tickmode\":\"array\",'tickvals': [min(y_sd),max(y_sd)],'ticktext': [\"Low uncertainty in predicted loading\",\"High uncertainty in predicted loading\"]},\n",
    "            surface_count=25,\n",
    "            ),ti+1,1)\n",
    "        \n",
    "        fig_standard_deviation.update(\n",
    "            layout_scene=dict(\n",
    "                xaxis_title=\"Gradient 1\",\n",
    "                yaxis_title=\"Gradient 2\",\n",
    "                zaxis_title=\"Gradient 3\",\n",
    "                aspectmode='data'\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # unscaledloadings = loadings[[\"Component {}\".format(i)]]\n",
    "        # unscaledloadings= unscaledloadings.sort_values(by=\"Component {}\".format(i),ascending=False)\n",
    "        # scaledloadings = MinMaxScaler().fit_transform(unscaledloadings).flatten().round(4)\n",
    "        # scaledloadings_col = MinMaxScaler(feature_range=(-10,10)).fit_transform(unscaledloadings).round(4)\n",
    "        # scaledloadings_col = pd.DataFrame(scaledloadings_col.T).apply(lambda x: 1 / (1 + math.exp(-x))).to_numpy()\n",
    "        # colours = sample_colorscale(\"RdBu\",samplepoints=scaledloadings_col)\n",
    "        # colour_dict = {x.split(\"_\")[0]:y for x,y in zip(unscaledloadings.index,colours)}\n",
    "        \n",
    "        # absolutescaledloadings = np.where(scaledloadings < 0.5, 1-scaledloadings,scaledloadings)\n",
    "        # rescaledloadings = MinMaxScaler().fit_transform(absolutescaledloadings.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # freq_dict = {x.split(\"_\")[0]:y for x,y in zip(unscaledloadings.index,rescaledloadings)}\n",
    "        # def color_func(word, *args, **kwargs): #colour function to supply to wordcloud function.. don't ask !\n",
    "            \n",
    "        #     return colour_dict[word]\n",
    "        \n",
    "        # wc = WordCloud(background_color=\"white\", color_func=color_func, \n",
    "        #             width=400, height=400, prefer_horizontal=1, \n",
    "        #             min_font_size=8, max_font_size=200\n",
    "        #             )\n",
    "        # # generate wordcloud from loadings in frequency dict\n",
    "        # wc = wc.generate_from_frequencies(freq_dict)\n",
    "        # im = wc.to_image()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # #im = Image.open(\"clouds/{}.png\".format(i+1))\n",
    "        # fig_estimated_mean.add_layout_image(\n",
    "        #         dict(\n",
    "        #             source=im,\n",
    "        #             xref=\"x\",\n",
    "        #             yref=\"y\",\n",
    "        #             x=0,\n",
    "        #             y=0,\n",
    "        #             xanchor=\"center\",\n",
    "        #             yanchor=\"middle\",\n",
    "        #             sizex=2,\n",
    "        #             sizey=2,\n",
    "                    \n",
    "        #             ),row=ti+1,col=2\n",
    "        # )\n",
    "        # fig_estimated_mean.update_xaxes(range=[-1,1],showticklabels=False,row=ti+1,col=2,showgrid=False,zeroline=False)\n",
    "        # fig_estimated_mean.update_yaxes(range=[-1,1],showticklabels=False,row=ti+1,col=2,showgrid=False,zeroline=False)\n",
    "\n",
    "        # fig_standard_deviation.add_layout_image(\n",
    "        #         dict(\n",
    "        #             source=im,\n",
    "        #             xref=\"x\",\n",
    "        #             yref=\"y\",\n",
    "        #             x=0,\n",
    "        #             y=0,\n",
    "        #             xanchor=\"center\",\n",
    "        #             yanchor=\"middle\",\n",
    "        #             sizex=2,\n",
    "        #             sizey=2,\n",
    "                    \n",
    "        #             ),row=ti+1,col=2\n",
    "        # )\n",
    "        # fig_standard_deviation.update_xaxes(range=[-1,1],showticklabels=False,row=ti+1,col=2,showgrid=False,zeroline=False)\n",
    "        # fig_standard_deviation.update_yaxes(range=[-1,1],showticklabels=False,row=ti+1,col=2,showgrid=False,zeroline=False)\n",
    "        ti += 1\n",
    "\n",
    "fig_estimated_mean.write_html(\"estimated_mean_PCA_axis.html\",default_width=2000,default_height=5000)\n",
    "fig_standard_deviation.write_html(\"standard_deviation_PCA_axis.html\",default_width=2000,default_height=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab6aaf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tasklabels,Taskindices=np.unique(data.Task_name,return_inverse=True)\n",
    "# FAC_TaskCentres=np.zeros([tasknum,4])\n",
    "# for i in range(tasknum):\n",
    "#     FAC_TaskCentres[i,:]=FAC[Taskindices==i,:].mean(axis=0)\n",
    "\n",
    "# Grad_TaskCentres=np.zeros([tasknum,3])\n",
    "# for i in range(tasknum):\n",
    "#     Grad_TaskCentres[i,:]=GRAD[Taskindices==i,:].mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "561d4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "k = 4\n",
    "kernel = 1.0 * Matern(length_scale=0.5, length_scale_bounds=(0.5, 1), nu=2.5) + WhiteKernel(noise_level_bounds=[0.001,0.5],noise_level=0.05)\n",
    "#kernel = 1.0 * Matern(length_scale=0.5, length_scale_bounds=(0.5, 1), nu=2.5)+ WhiteKernel(noise_level_bounds=[0.001,0.5],noise_level=0.05)\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=None,normalize_y=True,alpha=0.1)\n",
    "X=FAC_TaskCentres\n",
    "scores = {}\n",
    "\n",
    "for PCnum in range(Grad_TaskCentres.shape[1]):\n",
    "    standardscaler=StandardScaler()\n",
    "\n",
    "    y = Grad_TaskCentres[:,PCnum]\n",
    "    \n",
    "    @ignore_warnings(category=ConvergenceWarning)\n",
    "    def ___f():\n",
    "        score_gradfac, perm_scores_gradfac, pvalue_gradfac = permutation_test_score(gpr, X, y, scoring=\"neg_mean_absolute_error\", cv=kf, n_permutations=1000,n_jobs=-1)\n",
    "    \n",
    "        scores.update({PCnum:[score_gradfac,pvalue_gradfac]})\n",
    "    \n",
    "    ___f()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d619747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Results for component 0:\n",
      "        Largest loadings: \n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "Distracting_response    0.745\n",
       "Future_response         0.721\n",
       "Intrusive_response      0.610\n",
       "Focus_response          0.459\n",
       "Self_response           0.383\n",
       "Name: Component 0, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Permutation test score: -0.09940696353908357,\n",
      "        Permutation test significance: 0.1008991008991009\n",
      "        \n",
      "\n",
      "        Results for component 1:\n",
      "        Largest loadings: \n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "Detailed_response      0.773\n",
       "Deliberate_response    0.687\n",
       "Absorption_response    0.606\n",
       "Knowledge_response     0.535\n",
       "Problem_response       0.420\n",
       "Name: Component 1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Permutation test score: -0.20441239986895074,\n",
      "        Permutation test significance: 0.5184815184815185\n",
      "        \n",
      "\n",
      "        Results for component 2:\n",
      "        Largest loadings: \n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "Past_response         0.696\n",
       "Other_response        0.692\n",
       "Problem_response     -0.604\n",
       "Self_response         0.519\n",
       "Knowledge_response    0.387\n",
       "Name: Component 2, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Permutation test score: -0.21654914118065272,\n",
      "        Permutation test significance: 0.011988011988011988\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "numloadings = 5\n",
    "for score in scores:\n",
    "    try:\n",
    "        loading = loadings[\"Component {}\".format(score)]\n",
    "        loadingpos = loading.apply(lambda x: np.abs(x)).sort_values(ascending=False)\n",
    "        tops = loading[loadingpos[:numloadings].index]\n",
    "    except:\n",
    "        tops = \"Loadings from SPSS are unavailable\"\n",
    "    print(\n",
    "        \"\"\"\n",
    "        Results for component {}:\n",
    "        Largest loadings: \n",
    "        \"\"\".format(score)\n",
    "        )\n",
    "    with pd.option_context('display.max_rows', 5,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.width', 1000,\n",
    "                       'display.precision', 3,\n",
    "                       'display.colheader_justify', 'center'):\n",
    "        display(tops)\n",
    "    print(\n",
    "        \"\"\"\n",
    "        Permutation test score: {},\n",
    "        Permutation test significance: {}\n",
    "        \"\"\".format(scores[score][0],scores[score][1])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562bd96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6a264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bce8f376a3dc2c197604a06e7e05438ccd7ca7fbe84c8386480d864d293a56c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
